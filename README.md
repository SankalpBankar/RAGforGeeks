# ğŸ“šRAGforGeeks ğŸ¤–ğŸ” ğŸ‘“
A powerful Retrieval-Augmented Generation (RAG) assistant designed to provide accurate and contextual answers from GeeksforGeeks Nation SkillUp content.


## ğŸ“ Project Directory Structure ğŸ§ ğŸ’¬

```
RAGforGeeks-ChatBot/
â”‚â”€â”€ backend/
â”‚   â”‚â”€â”€ models/                         # Stores the Llama 2 model (download separately) ğŸ§ 
â”‚   â”‚â”€â”€ vectorstores/                    # Stores FAISS database ğŸ—„ï¸
â”‚   â”‚   â””â”€â”€ db_faiss/                     # FAISS index files ğŸ“
â”‚   â”‚â”€â”€ model.py                         # Main backend logic (LLM, FAISS retrieval) ğŸ”§
â”‚   â””â”€â”€ initialize_faiss.py              # Script to initialize FAISS (if needed) âš™ï¸
â”‚
â”‚â”€â”€ frontend/
â”‚   â”‚â”€â”€ app.py                           # Streamlit web interface for chatbot ğŸ–¥ï¸
â”‚
â”‚â”€â”€ setup.txt                            # Instructions to download the Llama 2 model ğŸ“
â”‚â”€â”€ requirements.txt                      # Dependencies for the project ğŸ“¦
â”‚â”€â”€ README.md                            # Project documentation ğŸ“š
```


## ğŸ’¡ Tech Stack ğŸ› ï¸
â€¢	Python ğŸ (Core programming language for backend logic)
â€¢	Llama 2 (GGML) ğŸ¦™ (Large Language Model used for answering queries locally)
â€¢	LangChain ğŸ”— (Framework for implementing RAG â€” Retrieval Augmented Generation)
â€¢	FAISS ğŸ” (For storing and searching vector embeddings efficiently)
â€¢	ChromaDB ğŸ—‚ï¸ (Optional vector store library used by LangChain)
â€¢	CTransformers âš¡ (Lightweight LLM inference for GGML models)
â€¢	Streamlit ğŸŒ (Frontend UI framework to interact with the chatbot)
â€¢	Groq API ğŸš€ (For high-speed LLM inference using GroqCloud)
â€¢	dotenv ğŸ” (To manage API keys securely via environment variables)


## âš™ï¸ Setup & Installation for RAGforGeeks ğŸ§ ğŸ’¬
Follow these steps to set up and run your RAG-based chatbot:
### 1ï¸âƒ£ Clone the Repository ğŸ“¥
```sh
git clone https://github.com/SankalpBankar/RAGforGeeks.git
cd RAGforGeeks
```

### 2ï¸âƒ£ Install Dependencies ğŸ“¦

Ensure you're using Python 3.9+, then install all required libraries:
```sh
pip install langchain groq chromadb tiktoken langchain-groq langchain-community pymupdf
```

### 3ï¸âƒ£ Configure Environment ğŸ”
Set your GROQ API key securely using an environment variable:
```sh
import os
os.environ["GROQ_API_KEY"] = "your-groq-api-key"
```
(Or use .env with python-dotenv if running locally.)

### 4ï¸âƒ£ Load the LLM (LLaMA3) ğŸ¦™
The code uses LangChain + Groq to load the model:
```sh
from langchain_groq import ChatGroq\n
llm = ChatGroq(\n
    groq_api_key=os.environ["GROQ_API_KEY"],
    model_name="llama3-8b-8192"
)
```

### 5ï¸âƒ£ Run Your Application ğŸš€
Ensure you have all backend files and Streamlit app ready. Then run:
```sh
streamlit run app.py
```

## ğŸ› ï¸ Troubleshooting ğŸš¨
â€¢	API Key Error
Set your key using:
```sh
os.environ["GROQ_API_KEY"] = "your-api-key"
```

â€¢	FAISS Index Empty or Missing
Run the ```sh initialize_faiss.py ``` script to embed and store documents.






