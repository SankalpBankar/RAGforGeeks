# ğŸ“šRAGforGeeks ğŸ¤–ğŸ” ğŸ‘“
A powerful Retrieval-Augmented Generation (RAG) assistant designed to provide accurate and contextual answers from GeeksforGeeks Nation SkillUp content.


## ğŸ“ Project Directory Structure ğŸ§ ğŸ’¬

```
RAGforGeeks-ChatBot/
â”‚â”€â”€ backend/
â”‚   â”‚â”€â”€ models/                         # Stores the Llama 2 model (download separately) ğŸ§ 
â”‚   â”‚â”€â”€ vectorstores/                    # Stores FAISS database ğŸ—„ï¸
â”‚   â”‚   â””â”€â”€ db_faiss/                     # FAISS index files ğŸ“
â”‚   â”‚â”€â”€ model.py                         # Main backend logic (LLM, FAISS retrieval) ğŸ”§
â”‚   â””â”€â”€ initialize_faiss.py              # Script to initialize FAISS (if needed) âš™ï¸
â”‚
â”‚â”€â”€ frontend/
â”‚   â”‚â”€â”€ app.py                           # Streamlit web interface for chatbot ğŸ–¥ï¸
â”‚
â”‚â”€â”€ setup.txt                            # Instructions to download the Llama 2 model ğŸ“
â”‚â”€â”€ requirements.txt                      # Dependencies for the project ğŸ“¦
â”‚â”€â”€ README.md                            # Project documentation ğŸ“š
```


## ğŸ’¡ Tech Stack ğŸ› ï¸
â€¢	Python ğŸ (Core programming language for backend logic)\n
â€¢	Llama 2 (GGML) ğŸ¦™ (Large Language Model used for answering queries locally)\n
â€¢	LangChain ğŸ”— (Framework for implementing RAG â€” Retrieval Augmented Generation)\n
â€¢	FAISS ğŸ” (For storing and searching vector embeddings efficiently)\n
â€¢	ChromaDB ğŸ—‚ï¸ (Optional vector store library used by LangChain)\n
â€¢	CTransformers âš¡ (Lightweight LLM inference for GGML models)\n
â€¢	Streamlit ğŸŒ (Frontend UI framework to interact with the chatbot)\n
â€¢	Groq API ğŸš€ (For high-speed LLM inference using GroqCloud)\n
â€¢	dotenv ğŸ” (To manage API keys securely via environment variables)\n


## âš™ï¸ Setup & Installation for RAGforGeeks ğŸ§ ğŸ’¬\n
Follow these steps to set up and run your RAG-based chatbot:\n
1ï¸âƒ£ Clone the Repository ğŸ“¥\n
git clone https://github.com/SankalpBankar/RAGforGeeks.git\n
cd RAGforGeeks\n

2ï¸âƒ£ Install Dependencies ğŸ“¦\n
Ensure you're using Python 3.9+, then install all required libraries:\n
pip install langchain groq chromadb tiktoken langchain-groq langchain-community pymupdf\n

3ï¸âƒ£ Configure Environment ğŸ”\n
Set your GROQ API key securely using an environment variable:\n
import os\n
os.environ["GROQ_API_KEY"] = "your-groq-api-key"
(Or use .env with python-dotenv if running locally.)\n

4ï¸âƒ£ Load the LLM (LLaMA3) ğŸ¦™\n
The code uses LangChain + Groq to load the model:\n
from langchain_groq import ChatGroq\n
llm = ChatGroq(\n
    groq_api_key=os.environ["GROQ_API_KEY"],\n
    model_name="llama3-8b-8192"\n
)

5ï¸âƒ£ Run Your Application ğŸš€\n
Ensure you have all backend files and Streamlit app ready. Then run:\n
streamlit run app.py\n

## ğŸ› ï¸ Troubleshooting ğŸš¨\n
â€¢	API Key Error\n
Set your key using:\n 
os.environ["GROQ_API_KEY"] = "your-api-key"\n

â€¢	FAISS Index Empty or Missing\n
Run the initialize_faiss.py script to embed and store documents.\n






